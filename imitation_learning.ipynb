{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtUkD1Ut4LWg"
   },
   "source": [
    "# TinyVLA architecture for PushT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur8HQACBXjZE"
   },
   "source": [
    "## Starting from https://medium.com/correll-lab/robotic-behavior-cloning-i-auto-regressive-transformers-a7be623f4291 and https://colab.research.google.com/drive/18GIHeOQ5DyjMN8iIRZL2EKZ0745NLIpg?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVKZ0ldlX55W",
    "outputId": "65e185f2-1d9e-4b55-8428-51f8220bee8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvips in e:\\miniconda\\envs\\pusht\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: cffi>=1.0.0 in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from pyvips) (1.17.1)\n",
      "Requirement already satisfied: pycparser in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from cffi>=1.0.0->pyvips) (2.21)\n",
      "Requirement already satisfied: zarr in e:\\miniconda\\envs\\pusht\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: asciitree in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from zarr) (0.3.3)\n",
      "Requirement already satisfied: numpy>=1.7 in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from zarr) (2.2.4)\n",
      "Requirement already satisfied: fasteners in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from zarr) (0.19)\n",
      "Requirement already satisfied: numcodecs>=0.6.4 in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from zarr) (0.10.2)\n",
      "Requirement already satisfied: entrypoints in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from numcodecs>=0.6.4->zarr) (0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from numcodecs>=0.6.4->zarr) (4.12.2)\n",
      "Requirement already satisfied: bitsandbytes in e:\\miniconda\\envs\\pusht\\lib\\site-packages (0.45.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from bitsandbytes) (2.6.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from bitsandbytes) (2.2.4)\n",
      "Requirement already satisfied: filelock in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\miniconda\\envs\\pusht\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "!pip install pyvips\n",
    "# !pip install gym-pusht\n",
    "# !pip install einops\n",
    "# !pip install \"accelerate>=0.26.0\"\n",
    "# !pip install matplotlib\n",
    "# !python --version\n",
    "# !pip install datasets\n",
    "# !pip install gdown\n",
    "!pip install zarr\n",
    "# !apt-get install -y libvips libvips-dev\n",
    "!apt-get install -y -qq libvips libvips-dev > /dev/null 2>&1\n",
    "!pip install peft -q\n",
    "!pip install -U bitsandbytes\n",
    "# !pip install flash-attn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7eJWrWLokFIm",
    "outputId": "a0823fd8-5948-4950-c1ad-8dd04b76f42d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'grep' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NLjH5M6r49V9"
   },
   "outputs": [],
   "source": [
    "import os # to deal with files\n",
    "import gdown # to download from google drive\n",
    "import zipfile # to unzip\n",
    "import zarr # to load the dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXTTdLhh4g0l",
    "outputId": "76e10e75-4c94-43fe-f82f-0e25f1df97c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HwSmigImXs8E"
   },
   "outputs": [],
   "source": [
    "# download demonstration data from Google Drive\n",
    "dataset_path = \"pusht_cchi_v7_replay.zarr.zip\"\n",
    "extracted_dataset_path = \"pusht_cchi_v7_replay.zarr\"  # Path to extracted dataset\n",
    "\n",
    "if not os.path.isfile(dataset_path):\n",
    "    id = \"1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\"\n",
    "    gdown.download(id=id, output=dataset_path, quiet=False)\n",
    "\n",
    "# Extract the dataset if it hasn't been extracted yet\n",
    "if not os.path.isdir(extracted_dataset_path):\n",
    "    with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pruuqvliXakg"
   },
   "outputs": [],
   "source": [
    "def create_sample_indices(\n",
    "        episode_ends:np.ndarray, sequence_length:int,\n",
    "        pad_before: int=0, pad_after: int=0):\n",
    "    indices = list()\n",
    "    for i in range(len(episode_ends)):\n",
    "        start_idx = 0\n",
    "        if i > 0:\n",
    "            start_idx = episode_ends[i-1]\n",
    "        end_idx = episode_ends[i]\n",
    "        episode_length = end_idx - start_idx\n",
    "\n",
    "        min_start = -pad_before\n",
    "        max_start = episode_length - sequence_length + pad_after\n",
    "\n",
    "        # range stops one idx before end\n",
    "        for idx in range(min_start, max_start+1):\n",
    "            buffer_start_idx = max(idx, 0) + start_idx\n",
    "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
    "            start_offset = buffer_start_idx - (idx+start_idx)\n",
    "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
    "            sample_start_idx = 0 + start_offset\n",
    "            sample_end_idx = sequence_length - end_offset\n",
    "            indices.append([\n",
    "                buffer_start_idx, buffer_end_idx,\n",
    "                sample_start_idx, sample_end_idx])\n",
    "    indices = np.array(indices)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def sample_sequence(train_data, sequence_length,\n",
    "                    buffer_start_idx, buffer_end_idx,\n",
    "                    sample_start_idx, sample_end_idx):\n",
    "    result = dict()\n",
    "    for key, input_arr in train_data.items():\n",
    "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
    "        data = sample\n",
    "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
    "            data = np.zeros(\n",
    "                shape=(sequence_length,) + input_arr.shape[1:],\n",
    "                dtype=input_arr.dtype)\n",
    "            if sample_start_idx > 0:\n",
    "                data[:sample_start_idx] = sample[0]\n",
    "            if sample_end_idx < sequence_length:\n",
    "                data[sample_end_idx:] = sample[-1]\n",
    "            data[sample_start_idx:sample_end_idx] = sample\n",
    "        result[key] = data\n",
    "    return result\n",
    "\n",
    "# normalize data\n",
    "def get_data_stats(data):\n",
    "    data = data.reshape(-1,data.shape[-1])\n",
    "    stats = {\n",
    "        'min': np.min(data, axis=0),\n",
    "        'max': np.max(data, axis=0)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "def normalize_data(data, stats):\n",
    "    # nomalize to [0,1]\n",
    "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
    "    # normalize to [-1, 1]\n",
    "    ndata = ndata * 2 - 1\n",
    "    return ndata\n",
    "\n",
    "def unnormalize_data(ndata, stats):\n",
    "    ndata = (ndata + 1) / 2\n",
    "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
    "    return data\n",
    "\n",
    "# dataset\n",
    "class PushTImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset_path: str,\n",
    "                 pred_horizon: int,\n",
    "                 obs_horizon: int,\n",
    "                 action_horizon: int,\n",
    "                 tokenizer = None):\n",
    "\n",
    "        # read from zarr dataset\n",
    "        dataset_root = zarr.open(dataset_path, 'r')\n",
    "\n",
    "        # float32, [0,1], (N,96,96,3)\n",
    "        train_image_data = dataset_root['data']['img'][:]\n",
    "        train_image_data = np.moveaxis(train_image_data, -1,1)\n",
    "        # (N,3,96,96)\n",
    "\n",
    "        # (N, D)\n",
    "        train_data = {\n",
    "            # first two dims of state vector are agent (i.e. gripper) locations\n",
    "            'agent_pos': dataset_root['data']['state'][:,:2],\n",
    "            'action': dataset_root['data']['action'][:]\n",
    "        }\n",
    "        episode_ends = dataset_root['meta']['episode_ends'][:]\n",
    "\n",
    "        # compute start and end of each state-action sequence\n",
    "        # also handles padding\n",
    "        indices = create_sample_indices(\n",
    "            episode_ends=episode_ends,\n",
    "            sequence_length=pred_horizon,\n",
    "            pad_before=obs_horizon-1,\n",
    "            pad_after=action_horizon-1)\n",
    "\n",
    "        # compute statistics and normalized data to [-1,1]\n",
    "        stats = dict()\n",
    "        normalized_train_data = dict()\n",
    "        for key, data in train_data.items():\n",
    "            stats[key] = get_data_stats(data)\n",
    "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
    "\n",
    "        # images are already normalized\n",
    "        normalized_train_data['image'] = train_image_data\n",
    "\n",
    "        self.indices = indices\n",
    "        self.stats = stats\n",
    "        self.normalized_train_data = normalized_train_data\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.action_horizon = action_horizon\n",
    "        self.obs_horizon = obs_horizon\n",
    "\n",
    "        # fixed prompt\n",
    "        prompt = \"Given a (96,96,3) RGB image, where the green T represents the goal state of the gray T block and the blue dot represents the robot's current position, determine the next [x, y] coordinates the robot should move toward. The goal is to push the gray T block onto the goal state in the same position and orientation as the green indication. Return only the next step.\"\n",
    "        self.tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "        # self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the start/end indices for this datapoint\n",
    "        buffer_start_idx, buffer_end_idx, \\\n",
    "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
    "\n",
    "        # get normalized data using these indices\n",
    "        nsample = sample_sequence(\n",
    "            train_data=self.normalized_train_data,\n",
    "            sequence_length=self.pred_horizon,\n",
    "            buffer_start_idx=buffer_start_idx,\n",
    "            buffer_end_idx=buffer_end_idx,\n",
    "            sample_start_idx=sample_start_idx,\n",
    "            sample_end_idx=sample_end_idx\n",
    "        )\n",
    "\n",
    "        # discard unused observations\n",
    "        nsample['image'] = nsample['image'][:self.obs_horizon,:]\n",
    "        nsample['agent_pos'] = nsample['agent_pos'][:self.obs_horizon,:]\n",
    "\n",
    "        # add fixed prompt to every example\n",
    "        # tokenized_prompt = self.tokenizer(self.prompt, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "        nsample['text'] = self.tokenized_prompt[\"input_ids\"].squeeze(0)\n",
    "        nsample['attn_mask'] = self.tokenized_prompt[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        return nsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zXpqaPfDdCKX"
   },
   "outputs": [],
   "source": [
    "def plt_img(image, pos=None, acts=None):\n",
    "    if hasattr(image, 'permute'):\n",
    "        image = image.permute(1, 2, 0).numpy()  # [96, 96, 3]\n",
    "\n",
    "    if image.dtype != 'uint8':\n",
    "        image = (image / 255.0)\n",
    "\n",
    "    # Plot the image\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, origin='upper')\n",
    "\n",
    "    if pos is not None:\n",
    "        # Handle a single 2D point\n",
    "        if pos.ndim == 1:\n",
    "            pos = pos.unsqueeze(0) if hasattr(pos, 'unsqueeze') else pos[None, :]\n",
    "        plt.plot(pos[:, 0], pos[:, 1], 'r+', label=\"Agent Position\")\n",
    "\n",
    "    if acts is not None:\n",
    "        # Handle a single 2D point\n",
    "        if acts.ndim == 1:\n",
    "            acts = acts.unsqueeze(0) if hasattr(acts, 'unsqueeze') else acts[None, :]\n",
    "        plt.plot(acts[:, 0], acts[:, 1], 'b*', label=\"Actions\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mHWGK5_azUFP"
   },
   "outputs": [],
   "source": [
    "# revision = \"2025-04-14\"\n",
    "revision = \"2024-04-02\"\n",
    "\n",
    "md2_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"vikhyatk/moondream2\",\n",
    "    revision=revision,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "md2_tokenizer.pad_token = md2_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoRuDqX8XfyJ",
    "outputId": "d745cdba-ef24-41fa-8157-22340617c1f5"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# parameters\n",
    "random_split = False\n",
    "batch_size = 2\n",
    "obs_horizon = 1\n",
    "action_horizon = 0\n",
    "pred_horizon = 1\n",
    "#|o|o|                             observations: 2\n",
    "#| |a|a|a|a|a|a|a|a|               actions executed: 8\n",
    "#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
    "\n",
    "# create dataset from file\n",
    "dataset = PushTImageDataset(\n",
    "    dataset_path=extracted_dataset_path,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon,\n",
    "    tokenizer = md2_tokenizer\n",
    ")\n",
    "# save training data statistics (min, max) for each dim\n",
    "stats = dataset.stats\n",
    "\n",
    "if random_split:\n",
    "  train_size = int(0.8 * len(dataset))\n",
    "  val_size = len(dataset) - train_size\n",
    "  train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "else:\n",
    "  train_dataset = Subset(dataset, range(int(0.8 * len(dataset))))\n",
    "  val_dataset = Subset(dataset, range(int(0.8 * len(dataset)), len(dataset)))\n",
    "\n",
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    # num_workers=1,\n",
    "    # persistent_workers=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    # num_workers=1,\n",
    "    # persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAuUEN1bXf1M",
    "outputId": "d81952aa-60a0-4209-fce2-c55a0249ad70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch['image'].shape: torch.Size([2, 1, 3, 96, 96])\n",
      "batch['agent_pos'].shape: torch.Size([2, 1, 2])\n",
      "batch['action'].shape torch.Size([2, 1, 2])\n",
      "batch['text'].shape torch.Size([2, 81])\n",
      "batch['attn_mask'].shape torch.Size([2, 81])\n"
     ]
    }
   ],
   "source": [
    "# visualize data in batch\n",
    "batch = next(iter(dataloader))\n",
    "print(\"batch['image'].shape:\", batch['image'].shape)\n",
    "print(\"batch['agent_pos'].shape:\", batch['agent_pos'].shape)\n",
    "print(\"batch['action'].shape\", batch['action'].shape)\n",
    "print(\"batch['text'].shape\", batch['text'].shape)\n",
    "print(\"batch['attn_mask'].shape\", batch['attn_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nydgJsgSc2kw"
   },
   "outputs": [],
   "source": [
    "vis_data = False\n",
    "# Visualizing dataset:\n",
    "\n",
    "if vis_data:\n",
    "  B=35\n",
    "  stats = {'min': 0, 'max': 96}\n",
    "  image = batch['image'][B][0]\n",
    "  pos = unnormalize_data(batch['agent_pos'][B], stats)\n",
    "  acts = unnormalize_data(batch['action'][B], stats)\n",
    "  plt_img(image, pos, acts)\n",
    "\n",
    "  # image = batch['image'][B][1]\n",
    "  # plt_img(image, pos, acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "27bxmH1EYMCt",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "flash_attn = False\n",
    "fourbit = False\n",
    "\n",
    "\n",
    "# https://huggingface.co/vikhyatk/moondream2/tree/2025-04-14\n",
    "# https://github.com/vikhyat/moondream\n",
    "# https://github.com/vikhyat/moondream/blob/main/notebooks/RepEng.ipynb\n",
    "# https://github.com/vikhyat/moondream/blob/main/moondream/finetune/finetune_region.py\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"vikhyatk/moondream2\",\n",
    "    revision=revision,\n",
    "    trust_remote_code=True,\n",
    "    device_map= {\"\": \"cuda\"}, # \"auto\",\n",
    "    quantization_config=bnb_config if fourbit else None,\n",
    "    attn_implementation=\"flash_attention_2\" if flash_attn else None\n",
    ")\n",
    "model.text_model.lm_head = torch.nn.Linear(in_features=2048, out_features=2)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aM0JlGdcdmg"
   },
   "source": [
    "### Example inference on base model before finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0M9NN2iOYNUt"
   },
   "outputs": [],
   "source": [
    "vis_inf = False\n",
    "\n",
    "if vis_inf:\n",
    "  prompt = \"Given a (96,96,3) RGB image, where the green T represents the goal state of the gray T block and the blue dot represents the robot's current position, determine the next [x, y] coordinates the robot should move toward. The goal is to push the gray T block onto the goal state in the same position and orientation as the green indication. Return only the next step.\"\n",
    "\n",
    "  img = Image.fromarray(image.permute(1, 2, 0).byte().numpy())\n",
    "\n",
    "  encoded_image = model.encode_image(img)\n",
    "  action = model.query(encoded_image, prompt)\n",
    "  print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QzCxkP8oYNXH"
   },
   "outputs": [],
   "source": [
    "if vis_inf:\n",
    "  # acts=[0.25,0.42]\n",
    "  acts=[0.14*96,0.14*96]\n",
    "  # for i, val in enumerate(acts):\n",
    "  #   acts[i] = unnormalize_data(val, stats)\n",
    "  acts = torch.tensor(acts)\n",
    "  plt_img(image, acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffphKbc8dpvk"
   },
   "source": [
    "### Finetuning with LoRA:\n",
    "\n",
    "https://www.youtube.com/watch?v=5rH_VjKXuzg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jEZRsZ-xgMUb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameter_verbosity = \"none\" # unique # all\n",
    "# Look at model parameter names to determine target_modules\n",
    "\n",
    "if parameter_verbosity == \"unique\":\n",
    "  unique_layers = set()\n",
    "  for name, _ in model.named_parameters():\n",
    "      layer_type = name.split(\".\")[-2]  # Extract second-to-last part (usually \"mlp\", \"ln\", etc.)\n",
    "      unique_layers.add(layer_type)\n",
    "  for layer in sorted(unique_layers):\n",
    "      print(layer)\n",
    "elif parameter_verbosity == \"all\":\n",
    "  for name, _ in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LB2uFo9pCaBg",
    "outputId": "8d1694f7-a2f5-4852-cfa2-6fd317c01916"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "target_modules = set()\n",
    "\n",
    "vision_suffixes = {'fc1', 'fc2', 'proj', 'qkv', 'proj_mlp.'}\n",
    "for name, _ in model.named_modules():\n",
    "    if \"visual.\" in name: # or \".region.\"\n",
    "        if any(name.endswith(suffix) for suffix in vision_suffixes):\n",
    "            target_modules.add(name)\n",
    "\n",
    "# target_modules.add(\"lm_head\")\n",
    "\n",
    "# print(\"Final target_modules for vision-only LoRA:\")\n",
    "# target_modules = sorted(target_modules)\n",
    "# for tm in target_modules:\n",
    "#     print(\" \", tm)\n",
    "len(target_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kCuZJnSObyq9"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
    "\n",
    "# 4 bit quantization\n",
    "if fourbit:\n",
    "  model.gradient_checkpointing_enable()\n",
    "  model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora = True\n",
    "lora_alpha = 32 # affects contribution of LoRA updates\n",
    "lora_rank = 64 # how much the LoRA layers compress the full-rank weight updates\n",
    "\n",
    "if lora:\n",
    "  lora_config = LoraConfig(\n",
    "      r = lora_rank,\n",
    "      lora_alpha = lora_alpha,\n",
    "      target_modules= target_modules,\n",
    "      lora_dropout = 0.1, # regularization\n",
    "      bias = \"none\",\n",
    "      task_type=\"CAUSAL_LM\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1AogG8PcsUP",
    "outputId": "df0cffd9-5132-4052-970e-f58453082e02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 30,799,872 || all params: 1,783,373,682 || trainable%: 1.7271\n"
     ]
    }
   ],
   "source": [
    "if lora:\n",
    "  model = get_peft_model(model, lora_config) # adds LoRA layers and freezes other layers\n",
    "  model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVppPhxedMR0",
    "outputId": "737627f4-2216-4956-cb0e-b368b2af9038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate scaling of 4.0 for  LoRA adapters\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "grad_accum_steps = 4\n",
    "lr = 1.53e-5\n",
    "if lora:\n",
    "  lr_scaling = lora_alpha / (lora_rank**0.5)\n",
    "  print(\"Learning rate scaling of\", lr_scaling, \"for  LoRA adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JErTLQp6g2az"
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "if lora:\n",
    "  lora_params = [p for n, p in model.named_parameters() if \"lora\" in n and p.requires_grad]\n",
    "  optimizer = torch.optim.AdamW(lora_params, lr=lr)\n",
    "else:\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "SbfFHVCSj15J"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "def compute_loss(batch):\n",
    "    images = batch[\"image\"].to(device).squeeze(1)\n",
    "    text = batch[\"text\"].to(device)\n",
    "    labels = batch[\"action\"].to(device).squeeze(1)\n",
    "    attn_mask = batch[\"attn_mask\"].to(device)\n",
    "    \n",
    "    pil_images = [to_pil_image(img) for img in images]\n",
    "    img_emb = model.vision_encoder(pil_images)\n",
    "    \n",
    "    text_emb = model.text_model.get_input_embeddings()(text)\n",
    "    \n",
    "    # input_embs = torch.cat((text_emb[:, 0:1, :], img_emb, text_emb[:, 1:, :]), dim=1)\n",
    "    input_embs = torch.cat((img_emb, text_emb[:, 1:, :], text_emb[:, 0:1, :]), dim=1) # [CLS]-esk token at the end\n",
    "    \n",
    "    outputs = model.text_model(\n",
    "        inputs_embeds=input_embs,\n",
    "        # labels=labels,\n",
    "        # attention_mask = attn_mask\n",
    "    )\n",
    "\n",
    "    print(outputs.logits[:,-1])\n",
    "\n",
    "    return loss_fn(outputs.logits[:,-1], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.4885, -1.2377],\n",
      "        [ 3.3657, -1.1319]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(9.4760, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "images = batch[\"image\"].to(device).squeeze(1)\n",
    "text = batch[\"text\"].to(device)\n",
    "labels = batch[\"action\"].to(device).squeeze(1)\n",
    "attn_mask = batch[\"attn_mask\"].to(device)\n",
    "\n",
    "pil_images = [to_pil_image(img) for img in images]\n",
    "img_emb = model.vision_encoder(pil_images)\n",
    "\n",
    "text_emb = model.text_model.get_input_embeddings()(text)\n",
    "\n",
    "input_embs = torch.cat((img_emb, text_emb[:, 1:, :], text_emb[:, 0:1, :]), dim=1) # [CLS]-esk token at the end\n",
    "\n",
    "outputs = model.text_model(inputs_embeds=input_embs)\n",
    "print(outputs.logits[:,-1])\n",
    "loss = loss_fn(outputs.logits[:,-1], labels)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 729, 2048])\n",
      "torch.Size([2, 81, 2048])\n",
      "torch.Size([2, 810, 2048])\n",
      "torch.Size([2, 810, 2])\n"
     ]
    }
   ],
   "source": [
    "print(img_emb.shape)\n",
    "print(text_emb.shape)\n",
    "print(input_embs.shape)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "BeJEwNlQ4BG5",
    "outputId": "9e791600-f3a3-438d-e90b-48df9dda3ac8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.4885, -1.2377],\n",
      "        [ 3.3657, -1.1319]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(9.4760, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.text_model.transformer.gradient_checkpointing_enable()\n",
    "# model.vision_encoder.transformer.gradient_checkpointing_enable()\n",
    "model.vision_encoder;\n",
    "# model.text_model\n",
    "# model.transformer.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "TLv8dONDYNZe",
    "outputId": "f1eb12c4-c3ec-44f2-b6d0-f86b7f51d8e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...   | 0/10178 [00:00<?, ?it/s]\n",
      "E:\\miniconda\\envs\\pusht\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "model.train()\n",
    "model.text_model.transformer.gradient_checkpointing_enable()\n",
    "\n",
    "i = 0\n",
    "for epoch in range(num_epochs):\n",
    "  for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "      i += 1\n",
    "\n",
    "      loss = compute_loss(batch)\n",
    "      loss.backward()\n",
    "\n",
    "      if i % grad_accum_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "      lr = lr_schedule(i / grad_accum_steps, total_steps)\n",
    "      for param_group in optimizer.param_groups:\n",
    "        if lora and param_group['params'] == lora_params:\n",
    "          param_group['lr'] = lr * lr_scaling\n",
    "        else:\n",
    "          param_group['lr'] = lr\n",
    "\n",
    "      if i % eval_steps == 0:\n",
    "        val_loss = 0\n",
    "        for val_batch in tqdm(val_dataloader, desc=\"Validatioon\"):\n",
    "          with torch.no_grad():\n",
    "            val_loss += compute_loss(val_batch).item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        print(val_loss)\n",
    "\n",
    "        # Save model\n",
    "        rand_id = random.randint(10000, 99999)\n",
    "        filename = f\"model_{rand_id}.pt\"\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        print(f\"Saved model to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
