{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nc0g2NLpUSGr"
   },
   "source": [
    "# Fine-tune SmolVLM on Visual Question Answering using Consumer GPU with QLoRA\n",
    "\n",
    "In this notebook we will fine-tune SmolVLM VQAv2 dataset. With this notebook you can also fine-tune Idefics3, since both models have the same model class/architecture.\n",
    "\n",
    "We will use some techniques in this notebook that will let you fine-tune the model on L4 with batch size of 4 only using around 16.4 GB of VRAM. We ran this notebook in that setup to test, but because we were able to afford A100 this notebook was last ran on an A100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/smolvlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in e:\\miniconda\\envs\\pusht\\lib\\site-packages (1.11.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q accelerate datasets peft bitsandbytes tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XyJaqZZ3uYYl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"MAX_JOBS\"] = \"2\"\n",
    "# !pip install -v flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAeMA0heVBjT"
   },
   "source": [
    "We will push out model to Hub so we need to authenticate ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yKd5xtSGj7cm"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRq8ve-LVAzU"
   },
   "source": [
    "In this notebook we will not do full fine-tuning but use QLoRA method, which loads an adapter to the quantized version of the model, saving space. If you want to do full fine-tuning, set `USE_LORA` and `USE_QLORA` to False. If you want to do LoRA, set `USE_QLORA`Â to False and `USE_LORA`Â to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "23d3d175e6e642c7abc2bce09b73cf4d",
      "db6ca8f47f274464b135909c907c946a",
      "d05822c6293c424fbf9df6ec0f6b532b",
      "05582fca18f443d6965776a721e30e9f",
      "3d8974fd1ba9415c8070c1eab8ad75cb",
      "648257c1b1c24e25a26355bddf75aa41",
      "afa9a31c6b7f45e082ae07dea4a2600e",
      "92232af543a4446cac53e4fcf3f4b6e1",
      "a5f06e59634f4edf9f3d9409846a2b31",
      "7ddfa8718bc24882ba2b50a899656107",
      "5983728a1c1e43edb4d16bee6ad40171",
      "dff574197f1f4466abb0eb46d36b8378"
     ]
    },
    "id": "b9CDMq0duYYn",
    "outputId": "65a4a5fa-fe4d-4243-b2d7-405a8aa81c04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda\\envs\\pusht\\lib\\site-packages\\peft\\mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "E:\\miniconda\\envs\\pusht\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10536960, 2256809840)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, Idefics3ForConditionalGeneration\n",
    "\n",
    "USE_LORA = False\n",
    "USE_QLORA = True\n",
    "SMOL = True\n",
    "\n",
    "model_id = \"HuggingFaceTB/SmolVLM-Base\" if SMOL else \"HuggingFaceM4/Idefics3-8B-Llama3\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "\n",
    "if USE_QLORA or USE_LORA:\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],\n",
    "        use_dora=False if USE_QLORA else True,\n",
    "        init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "    lora_config.inference_mode = False\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "    model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config if USE_QLORA else None,\n",
    "        # _attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.add_adapter(lora_config)\n",
    "    model.enable_adapters()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(model.get_nb_trainable_parameters())\n",
    "else:\n",
    "    model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        # _attn_implementation=\"flash_attention_2\",\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # if you'd like to only fine-tune LLM\n",
    "    for param in model.model.vision_model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIVhpp0EyZO2"
   },
   "source": [
    "The model as is is holding 2.7 GB of GPU RAM ðŸ’—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMTtg3dl3NX2"
   },
   "source": [
    "##Â Loading the dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWHMWTSZ3Pyr"
   },
   "source": [
    "We will load a small portion of the VQAv2 dataset. We are loading a small portion of the model for education purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # to deal with files\n",
    "import gdown # to download from google drive\n",
    "import zipfile # to unzip\n",
    "import zarr # to load the dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download demonstration data from Google Drive\n",
    "dataset_path = \"pusht_cchi_v7_replay.zarr.zip\"\n",
    "extracted_dataset_path = \"pusht_cchi_v7_replay.zarr\"  # Path to extracted dataset\n",
    "\n",
    "if not os.path.isfile(dataset_path):\n",
    "    id = \"1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\"\n",
    "    gdown.download(id=id, output=dataset_path, quiet=False)\n",
    "\n",
    "# Extract the dataset if it hasn't been extracted yet\n",
    "if not os.path.isdir(extracted_dataset_path):\n",
    "    with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_indices(\n",
    "        episode_ends:np.ndarray, sequence_length:int,\n",
    "        pad_before: int=0, pad_after: int=0):\n",
    "    indices = list()\n",
    "    for i in range(len(episode_ends)):\n",
    "        start_idx = 0\n",
    "        if i > 0:\n",
    "            start_idx = episode_ends[i-1]\n",
    "        end_idx = episode_ends[i]\n",
    "        episode_length = end_idx - start_idx\n",
    "\n",
    "        min_start = -pad_before\n",
    "        max_start = episode_length - sequence_length + pad_after\n",
    "\n",
    "        # range stops one idx before end\n",
    "        for idx in range(min_start, max_start+1):\n",
    "            buffer_start_idx = max(idx, 0) + start_idx\n",
    "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
    "            start_offset = buffer_start_idx - (idx+start_idx)\n",
    "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
    "            sample_start_idx = 0 + start_offset\n",
    "            sample_end_idx = sequence_length - end_offset\n",
    "            indices.append([\n",
    "                buffer_start_idx, buffer_end_idx,\n",
    "                sample_start_idx, sample_end_idx])\n",
    "    indices = np.array(indices)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def sample_sequence(train_data, sequence_length,\n",
    "                    buffer_start_idx, buffer_end_idx,\n",
    "                    sample_start_idx, sample_end_idx):\n",
    "    result = dict()\n",
    "    for key, input_arr in train_data.items():\n",
    "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
    "        data = sample\n",
    "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
    "            data = np.zeros(\n",
    "                shape=(sequence_length,) + input_arr.shape[1:],\n",
    "                dtype=input_arr.dtype)\n",
    "            if sample_start_idx > 0:\n",
    "                data[:sample_start_idx] = sample[0]\n",
    "            if sample_end_idx < sequence_length:\n",
    "                data[sample_end_idx:] = sample[-1]\n",
    "            data[sample_start_idx:sample_end_idx] = sample\n",
    "        result[key] = data\n",
    "    return result\n",
    "\n",
    "# normalize data\n",
    "def get_data_stats(data):\n",
    "    data = data.reshape(-1,data.shape[-1])\n",
    "    stats = {\n",
    "        'min': np.min(data, axis=0),\n",
    "        'max': np.max(data, axis=0)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "def normalize_data(data, stats):\n",
    "    # nomalize to [0,1]\n",
    "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
    "    # normalize to [-1, 1]\n",
    "    ndata = ndata * 2 - 1\n",
    "    return ndata\n",
    "\n",
    "def unnormalize_data(ndata, stats):\n",
    "    ndata = (ndata + 1) / 2\n",
    "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
    "    return data\n",
    "\n",
    "# dataset\n",
    "class PushTImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset_path: str,\n",
    "                 pred_horizon: int,\n",
    "                 obs_horizon: int,\n",
    "                 action_horizon: int,\n",
    "                 tokenizer = None):\n",
    "\n",
    "        # read from zarr dataset\n",
    "        dataset_root = zarr.open(dataset_path, 'r')\n",
    "\n",
    "        # float32, [0,1], (N,96,96,3)\n",
    "        train_image_data = dataset_root['data']['img'][:]\n",
    "        train_image_data = np.moveaxis(train_image_data, -1,1)\n",
    "        # (N,3,96,96)\n",
    "\n",
    "        # (N, D)\n",
    "        train_data = {\n",
    "            # first two dims of state vector are agent (i.e. gripper) locations\n",
    "            'agent_pos': dataset_root['data']['state'][:,:2],\n",
    "            'action': dataset_root['data']['action'][:]\n",
    "        }\n",
    "        episode_ends = dataset_root['meta']['episode_ends'][:]\n",
    "\n",
    "        # compute start and end of each state-action sequence\n",
    "        # also handles padding\n",
    "        indices = create_sample_indices(\n",
    "            episode_ends=episode_ends,\n",
    "            sequence_length=pred_horizon,\n",
    "            pad_before=obs_horizon-1,\n",
    "            pad_after=action_horizon-1)\n",
    "\n",
    "        # compute statistics and normalized data to [-1,1]\n",
    "        stats = dict()\n",
    "        normalized_train_data = dict()\n",
    "        for key, data in train_data.items():\n",
    "            stats[key] = get_data_stats(data)\n",
    "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
    "\n",
    "        # images are already normalized\n",
    "        normalized_train_data['image'] = train_image_data\n",
    "\n",
    "        self.indices = indices\n",
    "        self.stats = stats\n",
    "        self.normalized_train_data = normalized_train_data\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.action_horizon = action_horizon\n",
    "        self.obs_horizon = obs_horizon\n",
    "\n",
    "        # fixed prompt\n",
    "        prompt = \"Given a (96,96,3) RGB image, where the green T represents the goal state of the gray T block and the blue dot represents the robot's current position, determine the next [x, y] coordinates the robot should move toward. The goal is to push the gray T block onto the goal state in the same position and orientation as the green indication. Return only the next step.\"\n",
    "        self.tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "        # self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the start/end indices for this datapoint\n",
    "        buffer_start_idx, buffer_end_idx, \\\n",
    "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
    "\n",
    "        # get normalized data using these indices\n",
    "        nsample = sample_sequence(\n",
    "            train_data=self.normalized_train_data,\n",
    "            sequence_length=self.pred_horizon,\n",
    "            buffer_start_idx=buffer_start_idx,\n",
    "            buffer_end_idx=buffer_end_idx,\n",
    "            sample_start_idx=sample_start_idx,\n",
    "            sample_end_idx=sample_end_idx\n",
    "        )\n",
    "\n",
    "        # discard unused observations\n",
    "        nsample['image'] = nsample['image'][:self.obs_horizon,:]\n",
    "        nsample['agent_pos'] = nsample['agent_pos'][:self.obs_horizon,:]\n",
    "\n",
    "        # add fixed prompt to every example\n",
    "        # tokenized_prompt = self.tokenizer(self.prompt, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "        nsample['text'] = self.tokenized_prompt[\"input_ids\"].squeeze(0)\n",
    "        nsample['attn_mask'] = self.tokenized_prompt[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        return nsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# parameters\n",
    "random_split = False\n",
    "batch_size = 2\n",
    "obs_horizon = 1\n",
    "action_horizon = 0\n",
    "pred_horizon = 1\n",
    "#|o|o|                             observations: 2\n",
    "#| |a|a|a|a|a|a|a|a|               actions executed: 8\n",
    "#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
    "\n",
    "# create dataset from file\n",
    "dataset = PushTImageDataset(\n",
    "    dataset_path=extracted_dataset_path,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon,\n",
    "    tokenizer = md2_tokenizer\n",
    ")\n",
    "# save training data statistics (min, max) for each dim\n",
    "stats = dataset.stats\n",
    "\n",
    "if random_split:\n",
    "  train_size = int(0.8 * len(dataset))\n",
    "  val_size = len(dataset) - train_size\n",
    "  train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "else:\n",
    "  train_dataset = Subset(dataset, range(int(0.8 * len(dataset))))\n",
    "  val_dataset = Subset(dataset, range(int(0.8 * len(dataset)), len(dataset)))\n",
    "\n",
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    # num_workers=1,\n",
    "    # persistent_workers=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    # num_workers=1,\n",
    "    # persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POOqKqYRka5O"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('merve/vqav2-small', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Znf9vMo5rnSd"
   },
   "outputs": [],
   "source": [
    "split_ds = ds[\"validation\"].train_test_split(test_size=0.5)\n",
    "train_ds = split_ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIDioFlRuYYn",
    "outputId": "79b697a7-d245-4fdc-b0e8-d9ffa8627953"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['multiple_choice_answer', 'question', 'image'],\n",
       "    num_rows: 10717\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nwMO3n0X7Hv"
   },
   "source": [
    "Let's write our data collating function. We will apply prompt template to have questions and answers together so model can learn to answer. Then we pass the formatted prompts and images to the processor which processes both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0krVLZ-wNMl"
   },
   "outputs": [],
   "source": [
    "image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "            processor.tokenizer.additional_special_tokens.index(\"<image>\")]\n",
    "\n",
    "def collate_fn(examples):\n",
    "  texts = []\n",
    "  images = []\n",
    "  for example in examples:\n",
    "      image = example[\"image\"]\n",
    "      if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "      question = example[\"question\"]\n",
    "      answer = example[\"multiple_choice_answer\"]\n",
    "      messages = [\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                  {\"type\": \"text\", \"text\": \"Answer briefly.\"},\n",
    "                  {\"type\": \"image\"},\n",
    "                  {\"type\": \"text\", \"text\": question}\n",
    "              ]\n",
    "          },\n",
    "          {\n",
    "              \"role\": \"assistant\",\n",
    "              \"content\": [\n",
    "                  {\"type\": \"text\", \"text\": answer}\n",
    "              ]\n",
    "          }\n",
    "      ]\n",
    "      text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "      texts.append(text.strip())\n",
    "      images.append([image])\n",
    "\n",
    "  batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "  labels = batch[\"input_ids\"].clone()\n",
    "  labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "  labels[labels == image_token_id] = -100\n",
    "  batch[\"labels\"] = labels\n",
    "\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEYDjWpE3LD5"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvAs896cdwg8"
   },
   "source": [
    "We can now initialize `Trainer`Â and initialize `TrainingArguments`Â to pass to `Trainer`.\n",
    "\n",
    "Some notes:\n",
    "- If you use 8-bit QLoRA with the below setup it uses around 16.4 GB VRAM (beautiful, fits comfortably inside L4, Colab free tier)\n",
    "- We use gradient accumulation to simulate a larger batch size.\n",
    "- We also save up on memory from intermediate activations by using gradient checkpointing.\n",
    "\n",
    "**Disclaimer:**\n",
    "The techniques here aren't free lunch. The latter two will add additional compute to the training, thus slow down a bit (for reference on two A100s with bsz of 16, we were able to train for 2 hrs 43 mins with the gradient accumulation steps of 4, disabling it reduced it with 2 hr 35 mins).\n",
    "If you want to speed-up, you might play around, reduce to 4-bit precision and have a higher batch size. Note that 4-bit might result in model learning less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNE2yWAYrAhD"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    save_total_limit=1,\n",
    "    optim=\"paged_adamw_8bit\", # for 8-bit, keep this, else adamw_hf\n",
    "    bf16=True, #Â underlying precision for 8bit\n",
    "    output_dir=f\"./{model_name}-vqav2\",\n",
    "    hub_model_id=f\"{model_name}-vqav2\",\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBBSDpBhreJd",
    "outputId": "071ed677-1d9f-4f98-9d19-64834440c9c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QOCpw_-uYYo",
    "outputId": "7abb6937-c072-435a-c3f5-6dbb5b0b9eea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9/670 01:41 < 2:39:41, 0.07 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hN0QD9_uYYo"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "name": "Smol_VLM_FT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
